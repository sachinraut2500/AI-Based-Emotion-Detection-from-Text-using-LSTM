# -*- coding: utf-8 -*-
"""AI-Based Emotion Detection from Text using LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pKYgrwW1BbCxeOcU-7lO2w31ebT7-_yO
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.nn.utils.rnn import pad_sequence
from torchtext.vocab import GloVe
import re

# Simple dataset class
class EmotionDataset(Dataset):
    def __init__(self, texts, labels, vocab):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        tokens = [self.vocab.stoi.get(token, 0) for token in self.texts[idx]]
        return torch.tensor(tokens, dtype=torch.long), self.labels[idx]

def collate_fn(batch):
    texts, labels = zip(*batch)
    texts_padded = pad_sequence(texts, batch_first=True)
    labels = torch.tensor(labels)
    return texts_padded, labels

class LSTMEmotionClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, embeddings):
        super().__init__()
        self.embedding = nn.Embedding.from_pretrained(embeddings, freeze=False)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        embedded = self.dropout(self.embedding(x))
        _, (hidden, _) = self.lstm(embedded)
        out = self.fc(hidden[-1])
        return out

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    tokens = text.split()
    return tokens

def load_data():
    # Sample emotion dataset (replace with your own or larger dataset)
    data = {
        "text": [
            "I am so happy and excited",
            "This is so sad and depressing",
            "I am angry about the delay",
            "Feeling scared and anxious",
            "I love this amazing place",
            "I hate this terrible situation"
        ],
        "emotion": [
            "joy", "sadness", "anger", "fear", "joy", "anger"
        ]
    }
    df = pd.DataFrame(data)
    return df

def main():
    df = load_data()
    df['tokens'] = df['text'].apply(preprocess_text)

    label_enc = LabelEncoder()
    df['label'] = label_enc.fit_transform(df['emotion'])

    X_train, X_test, y_train, y_test = train_test_split(df['tokens'], df['label'], test_size=0.3, random_state=42)

    # Load GloVe embeddings
    glove = GloVe(name='6B', dim=100)

    train_dataset = EmotionDataset(list(X_train), list(y_train), glove.stoi)
    test_dataset = EmotionDataset(list(X_test), list(y_test), glove.stoi)

    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model = LSTMEmotionClassifier(vocab_size=len(glove.stoi),
                                  embed_dim=100,
                                  hidden_dim=64,
                                  output_dim=len(label_enc.classes_),
                                  embeddings=glove.vectors).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Training loop
    model.train()
    for epoch in range(10):
        total_loss = 0
        for texts, labels in train_loader:
            texts, labels = texts.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(texts)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}")

    # Evaluation
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for texts, labels in test_loader:
            texts, labels = texts.to(device), labels.to(device)
            outputs = model(texts)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f"Test Accuracy: {correct/total:.4f}")

if __name__ == "__main__":
    main()